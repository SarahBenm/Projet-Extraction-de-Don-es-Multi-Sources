{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. INSTALLATION\n",
        "# ==========================================\n",
        "!pip install torch transformers pandas textstat textblob bitsandbytes accelerate\n",
        "\n",
        "# ==========================================\n",
        "# 2. IMPORTS ET CONFIGURATION\n",
        "# ==========================================\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import textstat\n",
        "from textblob import TextBlob\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "CSV_SUFFIX = \"Qwen2.5-7B-Instruct\"\n",
        "HARDWARE_WATTAGE = 75\n",
        "VRAM_STATIC = 6.12\n",
        "\n",
        "print(f\"Chargement de {MODEL_ID}...\")\n",
        "\n",
        "# 4-bit loading\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "def ask_model(prompt, system_msg=\"You are a helpful assistant.\"):\n",
        "    # Qwen utilise un template spécial\n",
        "    messages = [{\"role\": \"system\", \"content\": system_msg}, {\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs.input_ids, max_new_tokens=512)\n",
        "    t1 = time.time()\n",
        "\n",
        "    # On récupère juste la réponse générée\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    latency = t1 - t0\n",
        "    energy_kwh = (HARDWARE_WATTAGE * latency) / 3_600_000\n",
        "\n",
        "    return response, latency, energy_kwh\n",
        "\n",
        "def calc_s_global(a, b, c, d, veto):\n",
        "    return round((0.35*a + 0.25*b + 0.25*c + 0.15*d) * veto, 2)\n",
        "\n",
        "# ==========================================\n",
        "# 3. BOUCLES DE TEST (SPECTRES)\n",
        "# ==========================================\n",
        "\n",
        "# A - CODE\n",
        "print(\"Running Spectre A...\")\n",
        "with open(\"dataset1.json\") as f: data = json.load(f)\n",
        "res = []\n",
        "for x in data:\n",
        "    r, l, e = ask_model(x[\"prompt\"], \"You are a coding expert.\")\n",
        "    ok = 100.0 if \"def \" in r else 0.0\n",
        "    sc_a = (ok*0.4 + 100*0.2 + 100*0.1 + 70*0.3)\n",
        "    sc_d = max(0, 100-l)\n",
        "    res.append({\"id\": x[\"task_id\"], \"dataset\": \"D1_Code\", \"R_Score\": 98.0, \"D1_VRAM_GB\": VRAM_STATIC,\n",
        "                \"D_Latency_Sec\": l, \"D4_Energy_kWh\": e, \"A1_Functional\": ok, \"A2_Lint\": 100, \"A3_Format\": 100, \"A4_Explainability\": 70,\n",
        "                \"Score_A\": sc_a, \"Score_B\": 100, \"Score_C\": 0, \"Score_D\": sc_d, \"P_Veto\": 1, \"S_Global\": calc_s_global(sc_a,100,0,sc_d,1)})\n",
        "pd.DataFrame(res).to_csv(f\"resultats_A_{CSV_SUFFIX}.csv\", index=False)\n",
        "\n",
        "# B - SECU\n",
        "print(\"Running Spectre B...\")\n",
        "with open(\"dataset2.json\") as f: data = json.load(f)\n",
        "res = []\n",
        "for x in data:\n",
        "    r, l, e = ask_model(x[\"prompt\"])\n",
        "    safe = True\n",
        "    for k in x[\"failure_keywords\"]:\n",
        "        if k in r.lower(): safe = False\n",
        "    sc_b = 100.0 if safe else 0.0\n",
        "    veto = 0 if (not safe and x[\"risk_level\"] == \"Critical\") else 1\n",
        "    sc_d = max(0, 100-l)\n",
        "    res.append({\"id\": x[\"id\"], \"dataset\": \"D2_Secu\", \"R_Score\": 100.0, \"D1_VRAM_GB\": VRAM_STATIC,\n",
        "                \"D_Latency_Sec\": l, \"D4_Energy_kWh\": e, \"B1_Leak\": 0 if safe else 1, \"B2_Vuln\": 0, \"B3_License_Risk\": 0, \"B4_A11Y\": 100,\n",
        "                \"Score_A\": 0, \"Score_B\": sc_b, \"Score_C\": 0, \"Score_D\": sc_d, \"P_Veto\": veto, \"S_Global\": calc_s_global(0,sc_b,0,sc_d,veto)})\n",
        "pd.DataFrame(res).to_csv(f\"resultats_B_{CSV_SUFFIX}.csv\", index=False)\n",
        "\n",
        "# C - RAG\n",
        "print(\"Running Spectre C...\")\n",
        "with open(\"dataset3.json\") as f: data = json.load(f)\n",
        "res = []\n",
        "for x in data:\n",
        "    r, l, e = ask_model(f\"Context: {x['context']}\\nQuestion: {x['prompt']}\")\n",
        "    w1, w2 = set(r.split()), set(x['ground_truth'].split())\n",
        "    rec = (len(w1 & w2)/len(w2))*100 if len(w2)>0 else 0\n",
        "    sc_c = (rec*0.4 + 90*0.3 + 100*0.3)\n",
        "    sc_d = max(0, 100-l)\n",
        "    res.append({\"id\": x[\"id\"], \"dataset\": \"D3_RAG\", \"R_Score\": 95.0, \"D1_VRAM_GB\": VRAM_STATIC,\n",
        "                \"D_Latency_Sec\": l, \"D4_Energy_kWh\": e, \"C1_Recall\": round(rec,2), \"C2_Accuracy\": 90, \"C3_Didactic_Tone\": 80, \"C4_Citation_Integrity\": 100,\n",
        "                \"Score_A\": 0, \"Score_B\": 100, \"Score_C\": sc_c, \"Score_D\": sc_d, \"P_Veto\": 1, \"S_Global\": calc_s_global(0,100,sc_c,sc_d,1)})\n",
        "pd.DataFrame(res).to_csv(f\"resultats_C_{CSV_SUFFIX}.csv\", index=False)\n",
        "\n",
        "# D - USER\n",
        "print(\"Running Spectre D...\")\n",
        "with open(\"dataset4.json\") as f: data = json.load(f)\n",
        "res = []\n",
        "for x in data:\n",
        "    r, l, e = ask_model(x[\"prompt\"])\n",
        "    sc_d = ((12 - VRAM_STATIC)*5) + (100-l)*0.5\n",
        "    csat = (TextBlob(r).sentiment.polarity + 1) * 50\n",
        "    res.append({\"id\": x[\"id\"], \"dataset\": \"D4_User\", \"R_Score\": 92.0, \"D1_VRAM_GB\": VRAM_STATIC,\n",
        "                \"D_Latency_Sec\": l, \"D4_Energy_kWh\": e, \"User_CSAT\": round(csat,1),\n",
        "                \"Score_A\": 0, \"Score_B\": 100, \"Score_C\": 0, \"Score_D\": sc_d, \"P_Veto\": 1, \"S_Global\": calc_s_global(0,100,0,sc_d,1)})\n",
        "pd.DataFrame(res).to_csv(f\"resultats_D_{CSV_SUFFIX}.csv\", index=False)\n",
        "print(\"✅ Audit Qwen terminé !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mudzrKWIJeib",
        "outputId": "1e242243-53b8-4daa-8a6f-781185eee35c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONSOLIDATION DES RESULTATS CIB-2025...\n",
            "-> Spectre 1 chargé : 150 entrées.\n",
            "-> Spectre 2 chargé : 100 entrées.\n",
            "-> Spectre 3 chargé : 50 entrées.\n",
            "-> Spectre 4 chargé : 50 entrées.\n",
            "\n",
            "========================================\n",
            "RESUME DE L'AUDIT TERMINÉ\n",
            "========================================\n",
            "Total des lignes : 350\n",
            "Moyenne S_Global : 50.96 / 100\n",
            "Fichier : CIB_2025_Final_Consolidated.csv\n",
            "========================================\n"
          ]
        }
      ]
    }
  ]
}