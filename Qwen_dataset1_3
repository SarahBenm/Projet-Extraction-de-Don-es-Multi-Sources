# === 1. INSTALLATION ===
#!pip install -q transformers bitsandbytes accelerate pylint textstat rouge-score pandas

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import json
import re
import textstat
import pandas as pd
import time
import io
import sys
import contextlib
from rouge_score import rouge_scorer

# === 2. CONFIGURATION (A CHANGER) ===
# CHOIX : "Qwen/Qwen2.5-7B-Instruct", "meta-llama/Meta-Llama-3-8B-Instruct", etc.
MODEL_ID = "Qwen/Qwen2.5-7B-Instruct" 
HF_TOKEN = "VOTRE_TOKEN_ICI" # Si n√©cessaire

print(f"Chargement de {MODEL_ID} en 4-bit...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map="auto", token=HF_TOKEN)

# === 3. FONCTIONS ===
def generate_response(prompt, context=None, system_role="You are a helpful assistant."):
    full_prompt = f"CONTEXTE:\n{context}\n\nQUESTION:\n{prompt}" if context else prompt
    messages = [{"role": "system", "content": system_role}, {"role": "user", "content": full_prompt}]
    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
    start = time.time()
    outputs = model.generate(inputs, max_new_tokens=512, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True), time.time() - start

def extract_code(response):
    match = re.search(r'```python(.*?)```', response, re.DOTALL)
    return match.group(1).strip() if match else response

# === 4. AUDIT SPECTRE A (CODE) ===
def audit_spectre_A():
    print(f"\nüöÄ D√©marrage Audit SPECTRE A (Dataset 1) pour {MODEL_ID}...")
    try:
        with open('dataset1.json', 'r') as f: data = json.load(f)
    except FileNotFoundError:
        print("‚ùå ERREUR: dataset1.json introuvable. Veuillez l'importer."); return pd.DataFrame()

    results = []
    for i, task in enumerate(data):
        print(f"[{i+1}/{len(data)}] Task {task['task_id']}...", end=" ")
        
        try:
            code_resp, lat = generate_response(task['prompt'], system_role="Tu es un expert Python. Donne UNIQUEMENT le code.")
            extracted = extract_code(code_resp)
            success = False
            
            # Ex√©cution silencieuse
            with io.StringIO() as buf, contextlib.redirect_stdout(buf):
                try:
                    exec_globals = {}
                    exec(extracted, exec_globals)
                    exec(task['test'], exec_globals)
                    test_func = task['test'].split('def ')[1].split('():')[0]
                    exec_globals[test_func]()
                    success = True
                except: success = False
            
            print("‚úÖ SUCC√àS" if success else "‚ùå √âCHEC")
            results.append({
                "id": task['task_id'], "success": success, 
                "readability": textstat.flesch_reading_ease(code_resp), "latency": lat
            })
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur critique : {e}")
            results.append({"id": task['task_id'], "success": False, "latency": 0})

    return pd.DataFrame(results)

# === 5. AUDIT SPECTRE C (RAG) ===
def audit_spectre_C():
    print(f"\nüöÄ D√©marrage Audit SPECTRE C (Dataset 3) pour {MODEL_ID}...")
    try:
        with open('dataset3.json', 'r') as f: data = json.load(f)
    except FileNotFoundError:
        print("‚ùå ERREUR: dataset3.json introuvable."); return pd.DataFrame()

    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
    results = []
    for i, item in enumerate(data):
        print(f"[{i+1}/{len(data)}] Doc {item['id']}...", end=" ")
        resp, lat = generate_response(item['prompt'], context=item['context'])
        score = scorer.score(item['ground_truth'], resp)['rouge1'].fmeasure
        print(f"Recall: {score:.2f}")
        results.append({"id": item['id'], "recall_score": score, "latency": lat})
    
    return pd.DataFrame(results)

# === 6. EXECUTION & SAUVEGARDE ===
base_name = MODEL_ID.split('/')[-1]

# Run A
df_A = audit_spectre_A()
if not df_A.empty:
    df_A.to_csv(f"resultats_A_{base_name}.csv", index=False)
    print(f"üíæ Sauvegard√© : resultats_A_{base_name}.csv")

# Run C
df_C = audit_spectre_C()
if not df_C.empty:
    df_C.to_csv(f"resultats_C_{base_name}.csv", index=False)
    print(f"üíæ Sauvegard√© : resultats_C_{base_name}.csv")

print("\nüéâ NOTEBOOK 1 TERMIN√â. T√©l√©chargez les fichiers CSV !")