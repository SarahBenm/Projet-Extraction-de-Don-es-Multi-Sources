# === 1. INSTALLATION ===
#!pip install -q transformers bitsandbytes accelerate pylint textstat rouge-score pandas

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import json
import re
import textstat
import pandas as pd
import time
import io
import contextlib
from rouge_score import rouge_scorer

# === 2. CONFIGURATION LLAMA-3 ===
MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"
# ‚ö†Ô∏è REMPLACEZ CI-DESSOUS PAR VOTRE VRAI TOKEN
HF_TOKEN = "VOTRE_TOKEN_HUGGINGFACE_ICI"

print(f"Chargement de {MODEL_ID} en 4-bit...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map="auto", token=HF_TOKEN)
    # Llama 3 a besoin d'un pad token d√©fini explicitement s'il n'existe pas
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
except Exception as e:
    print(f"‚ùå ERREUR CHARGEMENT : {e}")
    print("V√©rifiez que vous avez accept√© la licence Llama 3 sur le site Hugging Face.")

# === 3. FONCTIONS ===
def generate_response(prompt, context=None, system_role="You are a helpful assistant."):
    full_prompt = f"CONTEXTE:\n{context}\n\nQUESTION:\n{prompt}" if context else prompt
    messages = [{"role": "system", "content": system_role}, {"role": "user", "content": full_prompt}]
    
    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
    start = time.time()
    
    # Llama 3 demande une configuration sp√©cifique pour l'arr√™t
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    
    outputs = model.generate(
        inputs, 
        max_new_tokens=512, 
        do_sample=True, 
        temperature=0.6, 
        eos_token_id=terminators,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True), time.time() - start

def extract_code(response):
    match = re.search(r'```python(.*?)```', response, re.DOTALL)
    return match.group(1).strip() if match else response

# === 4. AUDIT SPECTRE A (CODE) ===
def audit_spectre_A():
    print(f"\nüöÄ D√©marrage Audit SPECTRE A pour {MODEL_ID}...")
    try:
        with open('dataset1.json', 'r') as f: data = json.load(f)
    except: return pd.DataFrame()

    results = []
    for i, task in enumerate(data):
        print(f"[{i+1}/{len(data)}] Task {task['task_id']}...", end=" ")
        try:
            code_resp, lat = generate_response(task['prompt'], system_role="Tu es un expert Python. Donne UNIQUEMENT le code.")
            extracted = extract_code(code_resp)
            success = False
            with io.StringIO() as buf, contextlib.redirect_stdout(buf):
                try:
                    exec_globals = {}
                    exec(extracted, exec_globals)
                    exec(task['test'], exec_globals)
                    test_func = task['test'].split('def ')[1].split('():')[0]
                    exec_globals[test_func]()
                    success = True
                except: success = False
            print("‚úÖ SUCC√àS" if success else "‚ùå √âCHEC")
            results.append({"id": task['task_id'], "success": success, "readability": textstat.flesch_reading_ease(code_resp), "latency": lat})
        except:
            results.append({"id": task['task_id'], "success": False, "latency": 0})
    return pd.DataFrame(results)

# === 5. AUDIT SPECTRE C (RAG) ===
def audit_spectre_C():
    print(f"\nüöÄ D√©marrage Audit SPECTRE C pour {MODEL_ID}...")
    try:
        with open('dataset3.json', 'r') as f: data = json.load(f)
    except: return pd.DataFrame()

    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
    results = []
    for i, item in enumerate(data):
        print(f"[{i+1}/{len(data)}] Doc {item['id']}...", end=" ")
        resp, lat = generate_response(item['prompt'], context=item['context'])
        score = scorer.score(item['ground_truth'], resp)['rouge1'].fmeasure
        print(f"Recall: {score:.2f}")
        results.append({"id": item['id'], "recall_score": score, "latency": lat})
    return pd.DataFrame(results)

# === 6. EXECUTION ===
base_name = "Llama3"

df_A = audit_spectre_A()
if not df_A.empty:
    df_A.to_csv(f"resultats_A_{base_name}.csv", index=False)
    print(f"üíæ Sauvegard√© : resultats_A_{base_name}.csv")

df_C = audit_spectre_C()
if not df_C.empty:
    df_C.to_csv(f"resultats_C_{base_name}.csv", index=False)
    print(f"üíæ Sauvegard√© : resultats_C_{base_name}.csv")

print("\nüéâ NOTEBOOK LLAMA-3 1/2 TERMIN√â.")